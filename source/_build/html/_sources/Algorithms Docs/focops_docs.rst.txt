First Order Constrained Optimization in Policy Space
====================================================

Introduction
------------

First Order Constrained Optimization in Policy
Space :raw-latex:`\cite{focops}` (FOCOPS) is a new
CPO :raw-latex:`\cite{cpo}`-based method which maximizes an agent’s
overall reward while ensuring the agent satisfies a set of cost
constraints. FOCOPS purposes that CPO has disadvantages below:

-  Sampling error resulting from taking sample trajectories from the
   current policy.

-  Approximation errors resulting from Taylor approximations.

-  Approximation errors resulting from using conjugate method to
   indirectly calculate the inverse of the Fisher information matrix.

Using a simple first-order method, FOCOPS shows that it is able to
eliminate the last two sources of error and outperform CPO. FOCOPS
mainly includes the following contributions: To begin with, it provide
two-stage policy update to optimize current policy. Next, it provides
the practical implementation for solving the two-stage policy update.
Finally, FOCOPS provides rigorous derivative proofs for the above
theories, as detailed in the appendix to this tutorial. One suggested
reading order is CPO, PCPO :raw-latex:`\cite{pcpo}` then FOCOPS. If you
haven’t read the PCPO yet, it doesn’t matter, it won’t affect your
reading experience much. But be sure to read this article after reading
the CPO tutorial we have written so that you can fully understand
following passage.

Target
------

In the previous chapters, you learned that CPO solves the following
optimization problems:

.. math::

   \begin{aligned}
       & \pi_{k+1}=\arg \max _{\pi \in \Pi_\theta} \E_{\substack{s \sim d_{\pi_k}\\a \sim \pi}}\left[A_{\pi_k}(s, a)\right]
       \label{FOCOPS:CPO_Target}\\
       \text{s.t.} \quad & J^{C}\left(\pi_k\right)+\frac{1}{1-\gamma} \E_{\substack{s \sim d_{\pi_k} \\ a \sim \pi}}\left[A^{C}_{\pi_k}(s, a)\right] \leq d \quad \label{FOCOPS:CPO_C_Target} \\
       & \bar{D}_{K L}\left(\pi \| \pi_k\right) \leq \delta\label{FOCOPS:CPO_D_Target}
   \end{aligned}

where :math:`\prod_{\theta}\subseteq\prod` denotes the set of
parametrized policies with parameters :math:`\theta`, and
:math:`\bar{D}_{K L}` is the KL divergence of two policy. In local
policy search for CMDPs, we additionally require policy iterates to be
feasible for the CMDP, so instead of optimizing over
:math:`\prod_{\theta}`, PCPO optimize over
:math:`\prod_{\theta}\cap\prod_{C}`. Next, we will introduce you to how
FOCOPS solves the above optimization problems. In order for you to have
a clearer understanding, we hope that you will read the next section
with the following questions:

-  What is two-stage policy update and how?

-  How to practically implement FOCOPS?

-  How parameters impact on the performance of the algorithm?

Two-stage policy update
-----------------------

Instead of solving Problem
`[FOCOPS:CPO_Target] <#FOCOPS:CPO_Target>`__-`[FOCOPS:CPO_D_Target] <#FOCOPS:CPO_D_Target>`__
directly, FOCOPS use a two-stage approach summarized below:

-  Given policy :math:`\pi_{\theta_k}`, find an *optimal update policy*
   :math:`\pi^*` by solving the optimization problem from Problem
   `[FOCOPS:CPO_Target] <#FOCOPS:CPO_Target>`__-`[FOCOPS:CPO_D_Target] <#FOCOPS:CPO_D_Target>`__
   in the nonparameterized policy space.

-  Project the policy found in the previous step back into the
   parameterized policy space :math:`\Pi_{\theta}` by solving for the
   closest policy :math:`\pi_{\theta}\in\Pi_{\theta}` to :math:`\pi^*`
   in order to obtain :math:`\pi_{\theta_{k+1}}`.

Finding the Optimal Update Policy
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the first stage, FOCOPS rewrites Problem
`[FOCOPS:CPO_Target] <#FOCOPS:CPO_Target>`__-`[FOCOPS:CPO_D_Target] <#FOCOPS:CPO_D_Target>`__
as below:

.. math::

   \begin{aligned}
       & \pi^*=\arg \max _{\pi \in \Pi} \E_{\substack{s \sim d_{\pi_k}\\a \sim \pi}}\left[A_{\pi_k}(s, a)\right]
       \label{FOCOPS:Target}\\
       \text{s.t.} \quad & J^{C}\left(\pi_k\right)+\frac{1}{1-\gamma} \E_{\substack{s \sim d_{\pi_k} \\ a \sim \pi}}\left[A^{C}_{\pi_k}(s, a)\right] \leq d \quad \label{FOCOPS:C_Target} \\
       & \bar{D}_{K L}\left(\pi \| \pi_k\right) \leq \delta\label{FOCOPS:D_Target}
   \end{aligned}

In fact, these problems are only slightly different from Problem
`[FOCOPS:CPO_Target] <#FOCOPS:CPO_Target>`__-`[FOCOPS:CPO_D_Target] <#FOCOPS:CPO_D_Target>`__,
that is the parameter of interest is now the nonparameterized policy
:math:`\pi` and not the policy parameter :math:`\theta`. Then FOCOPS
provides a solution as following:

.. container:: theorem

   Let
   :math:`\tilde{b}=(1-\gamma)\left(b-\tilde{J}^C\left(\pi_{\theta_k}\right)\right)`
   If :math:`\pi_{\theta_k}` is a feasible solution, the optimal policy
   for Problem
   `[FOCOPS:Target] <#FOCOPS:Target>`__-`[FOCOPS:D_Target] <#FOCOPS:D_Target>`__
   takes the form

   .. math::

      \label{FOCOPS:Theorem1_Eq1}
      \pi^*(a \mid s)=\frac{\pi_{\theta_k}(a \mid s)}{Z_{\lambda, \nu}(s)} \exp \left(\frac{1}{\lambda}\left(A_{\pi_{\theta_k}}(s, a)-\nu A^C_{\pi_{\theta_k}}(s, a)\right)\right)

   where :math:`Z_{\lambda,\nu}(s)` is the partition function which
   ensures Problem `[FOCOPS:Theorem1_Eq1] <#FOCOPS:Theorem1_Eq1>`__ is a
   valid probability distribution, :math:`\lambda` and :math:`\nu` are
   solutions to the optimization problem:

   .. math::

      \label{FOCOPS:Theorem1_Eq2}
      \min _{\lambda, \nu \geq 0} \lambda \delta+\nu \tilde{b}+\lambda \underset{\substack{s \sim d^{\pi_{\theta_k}} \\ a \sim \pi^*}}{\mathbb{E}}\left[\log Z_{\lambda, \nu}(s)\right]

The form of the optimal policy is intuitive, it gives high probability
mass to areas of the state-action space with high return which is offset
by a penalty term times the cost advantage. We will refer to the optimal
solution to Problem
`[FOCOPS:Target] <#FOCOPS:Target>`__-`[FOCOPS:D_Target] <#FOCOPS:D_Target>`__
as the *optimal update policy*. If you don’t quite understand the
meaning of the above equation, you can first think that FOCOPS finally
solves Problem
`[FOCOPS:Target] <#FOCOPS:Target>`__-`[FOCOPS:D_Target] <#FOCOPS:D_Target>`__
by solving Problem `[FOCOPS:Theorem1_Eq1] <#FOCOPS:Theorem1_Eq1>`__ and
`[FOCOPS:Theorem1_Eq2] <#FOCOPS:Theorem1_Eq2>`__. That is, Theorem
`[FOCOPS:Solving the optimal update policy] <#FOCOPS:Solving the optimal update policy>`__
is a viable solution. We have given a detailed proof of Theorem
`[FOCOPS:Solving the optimal update policy] <#FOCOPS:Solving the optimal update policy>`__
in the appendix, and reading it together with the body part can deepen
your understanding.

.. container:: tcolorbox

   | **Question:** what is the bound for FOCOPS worst-case guarantee for
     cost constraint?
   | **Answer:** FOCOPS purpose that the optimal update policy
     :math:`\pi^*` satisfies the following bound for worst-case
     guarantee for cost constraint in CPO:

     .. math:: J^C\left(\pi^*\right) \leq d+\frac{\sqrt{2 \delta} \gamma \epsilon_C^{\pi^*}}{(1-\gamma)^2}

     where
     :math:`\epsilon^C_{\pi^*}=\max _s\left|\mathbb{E}_{a \sim \pi}\left[A^C_{\pi_{\theta_k}}(s, a)\right]\right|`.
      
   |  
   | **Question:** Can FOCOPS solve the multi-constraint problem and
     how?
   | **Answer:** By introducing Lagrange multipliers
     :math:`\nu_1,\nu_2,...,\nu_m\ge0`, one for each cost constraint and
     applying a similar duality arguments, FOCOPS extend its results to
     accommodate for multiple constraints.

Approximating the Optimal Update Policy
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In previous section the optimal update policy :math:`\pi^*` is obtained,
however, it is not a parameterized policy. In this section we will show
you how FOCOPS project the optimal update policy back into the
parameterized policy space by minimizing the loss function:

.. math:: \mathcal{L}(\theta)=\underset{s \sim d^{\pi_{\theta_k}}}{\mathbb{E}}\left[D_{\mathrm{KL}}\left(\pi_\theta \| \pi^*\right)[s]\right]

Here :math:`\pi_{\theta}\in \Pi_{\theta}` is some projected policy which
FOCOPS will use to approximate the optimal update policy. the
first-order methods is also used to minimize this loss function:

.. container:: corollary

   The gradient of :math:`\mathcal{L}(\theta)` takes the form

   .. math::

      \label{FOCOPS:Gradient of L}
      \nabla_\theta \mathcal{L}(\theta)=\underset{s \sim d^{\pi_\theta}}{\mathbb{E}}\left[\nabla_\theta D_{K L}\left(\pi_\theta \| \pi^*\right)[s]\right]

   where

   .. math::

      \begin{aligned}
      & \nabla_\theta D_{K L}\left(\pi_\theta \| \pi^*\right)[s]=\nabla_\theta D_{K L}\left(\pi_\theta \| \pi_{\theta_k}\right)[s]\nonumber\\
      & -\frac{1}{\lambda} \underset{a \sim \pi_{\theta_k}}{\mathbb{E}}\left[\frac{\nabla_\theta \pi_\theta(a \mid s)}{\pi_{\theta_k}(a \mid s)}\left(A_{\pi_{\theta_k}}(s, a)-\nu A^C_{\pi_{\theta_k}}(s, a)\right)\right]
      \end{aligned}

*Proof.* See Appendix.

Note that Equation `[FOCOPS:Gradient of L] <#FOCOPS:Gradient of L>`__
can be estimated by sampling from the trajectories generated by policy
:math:`\pi_{\theta_k}` so policy can be trained using stochastic
gradients.

Corollary `[FOCOPS:corollary1] <#FOCOPS:corollary1>`__ provides an
outline for FOCOPS algorithm: At every iteration we begin with a policy
:math:`\pi_{\theta_k}`, which we use to run trajectories and gather
data. We use that data and Equation
`[FOCOPS:Theorem1_Eq2] <#FOCOPS:Theorem1_Eq2>`__ to first estimate
:math:`\lambda` and :math:`\nu`. We then draw a minibatch from the data
to estimate :math:`\nabla_\theta \mathcal{L}(\theta)` given in Corollary
`[FOCOPS:corollary1] <#FOCOPS:corollary1>`__. After taking a gradient
step using Equation `[FOCOPS:Gradient of L] <#FOCOPS:Gradient of L>`__,
we draw another minibatch and repeat the process.

practical implementation
------------------------

Solving Problem `[FOCOPS:Theorem1_Eq2] <#FOCOPS:Theorem1_Eq2>`__ is
computationally impractical for large state or action spaces as it
requires calculating the partition function :math:`Z_{\lambda,\nu}(s)`
which often involves evaluating a high-dimensional integral or sum.
Furthermore, :math:`\lambda` and :math:`\nu` depend on :math:`k` and
should be adapted at every iteration. So in this section we will
introduce you how FOCOPS practically implement its algorithm purposed.
In practice, FOCOPS found that a fixed :math:`\lambda` found through
hyperparameter sweeps provides good results, whcih means the value of
:math:`\lambda` does not have to be updated. However :math:`\nu` needs
to be continuously adapted during training so as to ensure cost
constraint satisfaction. FOCOPS appeals to an intuitive heuristic for
determining :math:`\nu` based on primal-dual gradient methods. With
strong duality, the optimal :math:`\lambda^*` and :math:`\nu^*`
minimizes the dual function
`[FOCOPS:Theorem1_Eq2] <#FOCOPS:Theorem1_Eq2>`__ which then be denoted
as :math:`L(\pi^*,\lambda,\nu)`. By applying gradient descent w.r.t
:math:`\nu` to minimize :math:`L(\pi^*,\lambda,\nu)`, we obtain:

.. container:: corollary

   The derivative of :math:`L(\pi^*,\lambda,\nu)` w.r.t :math:`\nu` is

   .. math::

      \label{FOCOPS:Eq for corollary2}
      \frac{\partial L\left(\pi^*, \lambda, \nu\right)}{\partial \nu}=\tilde{b}-\underset{\substack{s \sim d^{\pi^*} \\ a \sim \pi^*}}{\mathbb{E}}\left[A_{\pi_{\theta_k}}(s, a)\right]

*Proof.* See Appendix. The last term in the gradient expression in
Equation `[FOCOPS:Eq for corollary2] <#FOCOPS:Eq for corollary2>`__
cannot be evaluated since we do not have access to :math:`\pi^*`.
However since :math:`\pi_{\theta_k}` and :math:`\pi^*` are ’close’ (by
constraint `[FOCOPS:D_Target] <#FOCOPS:D_Target>`__), it is reasonable
to assume that
:math:`E_{s \sim d^{\pi_k}, a \sim \pi^*}\left[A_{\pi_{\theta_k}}(s, a)\right] \approx E_{s \sim d^{\pi_k}, a \sim \pi_{\theta_k}}\left[A_{\pi_{\theta_k}}(s, a)\right]=0`.
In practice this term can be set to zero which gives the update term:

.. math::

   \label{FOCOPS:update1}
   \nu \leftarrow \underset{\nu}{\operatorname{proj}}\left[\nu-\alpha\left(d-J^C\left(\pi_{\theta_k}\right)\right)\right]

where :math:`\alpha` is the step size. Note that we have incorporated
the discount term :math:`(1-\gamma)` into :math:`\tilde{b}` into the
step size. The projection operator :math:`proj_{\nu}` projects
:math:`\nu` back into the interval :math:`[0,\nu_{max}]` where
:math:`\nu_{max}` is chosen so that :math:`\nu` does not become too
large. In fact. FOCOPS purposed that even setting
:math:`\nu_{max}=+\infty` does not appear to greatly reduce performance.
Practically, :math:`J^C(\pi_{\theta_k})` can be estimated via Monte
Carlo methods using trajectories collected from :math:`\pi_{\theta_k}`.
Using the update rule `[FOCOPS:update1] <#FOCOPS:update1>`__, FOCOPS
performs one update step on :math:`\nu` before updating the policy
parameters :math:`\theta`. A per-state acceptance indicator function
:math:`I\left(s_j\right)^n:=\mathbf{1}_{D_{\mathrm{KL}}\left(\pi_\theta \| \pi_{\theta_k}\right)\left[s_j\right] \leq \delta}`
is added to `[FOCOPS:Gradient of L] <#FOCOPS:Gradient of L>`__ in order
to better enforce the accuracy for the purposed first-order method.

.. container:: tcolorbox

   | **Question:** Why the function :math:`I` can enforce the accuracy
     for the purposed first-order method?
   | **Answer:** Function :math:`I` rejects the sampled states whose
     :math:`D_{KL}(\pi_{\theta}||\pi_{\theta_k})` is too large from
     gradient update. If one sampled states violates the constraints of
     KL-divergence, it will not take part in gradient descent.

Finally the FOCOPS sample gradient update term is:

.. math::

   \label{FOCOPS:update2}
       \hat{\nabla}_\theta \mathcal{L}(\theta) \approx \frac{1}{N} \sum_{j=1}^N\left[\nabla_\theta D_{\mathrm{KL}}\left(\pi_\theta \| \pi_{\theta_k}\right)\left[s_j\right]-A^{\lambda}\right] I\left(s_j\right)

where
:math:`A^{\lambda}=\frac{1}{\lambda} \frac{\nabla_\theta \pi_\theta\left(a_j \mid s_j\right)}{\pi_{\theta_k}\left(a_j \mid s_j\right)}\left(\hat{A}\left(s_j, a_j\right)-\nu \hat{A}^C\left(s_j, a_j\right)\right)`

*Proof.* See Appendix.

Here :math:`N` is the number of samples collected by policy
:math:`\pi_{\theta_k}`, :math:`\hat A` and :math:`\hat A^C` are
estimates of the advantage functions (for the return and cost) obtained
from critic networks. The advantage functions is obtained by using the
Generalized Advantage Estimator (GAE) :raw-latex:`\cite{gae}`. Note that
FOCOPS only requires first order methods (gradient descent) and is thus
extremely simple to implement.

Variables Analysis
------------------

In this section we will explain the meaning of parameters
:math:`\lambda` and :math:`\mu` of FOCOPS and point out their impact on
the performance of the algorithm in the experiment. Reading this section
will give you a more intuitive understanding of the idea of FOCOPS.

Analysis of :math:`\lambda`
~~~~~~~~~~~~~~~~~~~~~~~~~~~

In Equation `[FOCOPS:Theorem1_Eq1] <#FOCOPS:Theorem1_Eq1>`__, note that
as :math:`\lambda \rightarrow 0`, :math:`\pi^*` approaches a greedy
policy; as :math:`\lambda` increase, the policy becomes more
exploratory. Therefore :math:`\lambda` is similar to the temperature
term used in maximum entropy reinforcement learning, which has been
shown to produce reasonable results when kept fixed during training. In
practice, FOCOPS finds that its algorithm reaches the best performance
when the :math:`\lambda` is fixed.

Analysis of :math:`\nu`
~~~~~~~~~~~~~~~~~~~~~~~

We recall that in Equation
`[FOCOPS:Theorem1_Eq1] <#FOCOPS:Theorem1_Eq1>`__, :math:`\nu` acts as a
cost penalty term where increasing :math:`\nu` makes it less likely for
state-action pairs with higher costs to be sampled by :math:`\pi^*`
Hence in this regard, the update rule in
`[FOCOPS:update1] <#FOCOPS:update1>`__ is intuitive in that it increases
:math:`\nu` if :math:`J^C(\pi_{\theta_k})>d` (which means the agent
violate the cost constraints) and decreases :math:`\nu` otherwise.

Appendix
--------

Proof for Theorem `[FOCOPS:Solving the optimal update policy] <#FOCOPS:Solving the optimal update policy>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. container:: lemma

   | Problem
     `[FOCOPS:Target] <#FOCOPS:Target>`__-`[FOCOPS:D_Target] <#FOCOPS:D_Target>`__
     is convex w.r.t
   | :math:`\pi={\pi(a|s):s\in \mathrm{S},a\in\mathrm{A}}`.

*Proof*. First note that the objective function is linear w.r.t
:math:`\pi`. Since :math:`J^{C}(\pi_{\theta_k})` is a constant w.r.t
:math:`\pi`, constraint `[FOCOPS:C_Target] <#FOCOPS:C_Target>`__ is
linear. Constraint `[FOCOPS:D_Target] <#FOCOPS:D_Target>`__ can be
rewritten as
:math:`\sum_s d^{\pi_{\theta_k}}(s) D_{\mathrm{KL}}\left(\pi \| \pi_{\theta_k}\right)[s] \leq \delta`.
The KL divergence is convex w.r.t its first argument, hence Constraint
`[FOCOPS:C_Target] <#FOCOPS:C_Target>`__ which is a linear combination
of convex functions is also convex. Since :math:`\pi_{\theta_k}`
satisfies Constraint `[FOCOPS:D_Target] <#FOCOPS:D_Target>`__ is also
satisfies Constraint `[FOCOPS:C_Target] <#FOCOPS:C_Target>`__, therefore
Slater’s constraint qualification holds, strong duality holds.

| 
| So the optimal value of Problem
  `[FOCOPS:Target] <#FOCOPS:Target>`__-`[FOCOPS:D_Target] <#FOCOPS:D_Target>`__
  :math:`p^*` can be solved by solving the corresponding dual problem,
  Let

  .. math:: L(\pi, \lambda, \nu)=\lambda \delta+\nu \tilde{b}+\underset{s \sim d^{\pi_{\theta_k}}}{\mathbb{E}}\left[A^{lag}-\lambda D_{\mathrm{KL}}\left(\pi \| \pi_{\theta_k}\right)[s]\right]\nonumber

  where
  :math:`A^{lag}=\underset{a \sim \pi(\cdot \mid s)}{\mathbb{E}}\left[A_{\pi_{\theta_k}}(s, a)-\nu A^C_{\pi_{\theta_k}}(s, a)\right]`.
  Therefore.

  .. math::

     \label{FOCOPS:lag_dual}
     p^*=\max _{\pi \in \Pi} \min _{\lambda, \nu \geq 0} L(\pi, \lambda, \nu)=\min _{\lambda, \nu \geq 0} \max _{\pi \in \Pi} L(\pi, \lambda, \nu)

  Note that if :math:`\pi^*`, :math:`\lambda^*`, :math:`\nu^*` are
  optimal for Problem\ `[FOCOPS:lag_dual] <#FOCOPS:lag_dual>`__,
  :math:`\pi^*` is also optimal for Problem
  `[FOCOPS:Target] <#FOCOPS:Target>`__-`[FOCOPS:D_Target] <#FOCOPS:D_Target>`__
  because of the strong duality.

Consider the inner maximization problem in Problem
`[FOCOPS:lag_dual] <#FOCOPS:lag_dual>`__, we separate it from the
original problem and try to solve it first:

.. math::

   \label{FOCOPS:lag1}
   \begin{array}{cl}
   \underset{\pi}{\operatorname{max}} & A^{lag}-\underset{a \sim \pi(\cdot \mid s)}{\mathbb{E}}\left[\lambda\left(\log \pi(a \mid s)+\log \pi_{\theta_k}(a \mid s)\right)\right] \\
   \text { s.t. } & \sum_a \pi(a \mid s)=1 \\
   & \pi(a \mid s) \geq 0 \quad \forall a \in \mathcal{A}
   \end{array}

which is equivalent to the inner maximization problem in
`[FOCOPS:lag_dual] <#FOCOPS:lag_dual>`__. This is clearly a convex
optimization problem which we can solve using a simple Lagrangian
argument. We can write the Lagrangian of it as:

.. math::

   \begin{aligned}
   G(\pi)=\sum_a \pi(a \mid s)\left[A_{\pi_{\theta_k}}(s, a)-\nu A^C_{\pi_{\theta_k}}(s, a)\right.\\
   \nonumber
   \left.-\lambda\left(\log \pi(a \mid s)-\log \pi_{\theta_k}(a \mid s)\right)+\zeta\right]-1
   \end{aligned}

where :math:`\zeta > 0` is the Lagrange multiplier associated with the
constraint :math:`\sum_a \pi(a \mid s)=1`. Different :math:`G(\pi)`
w.r.t. :math:`\pi(a \mid s)` for some :math:`a`:

.. math::

   \begin{aligned}
   \label{FOCOPS:D_lag_1}
   \frac{\partial G}{\partial \pi(a \mid s)}=A_{\pi_{\theta_k}}(s, a)-\nu A^C_{\pi_{\theta_k}}(s, a)\nonumber\\-\lambda\left(\log \pi(a \mid s)+1-\log \pi_{\theta_k}(a \mid s)\right)+\zeta
   \end{aligned}

Setting Equation\ `[FOCOPS:D_lag_1] <#FOCOPS:D_lag_1>`__ to zero and
rearranging the term, we obtain:

.. math:: \pi(a \mid s)=\pi_{\theta_k}(a \mid s) \exp \left(\frac{1}{\lambda}\left(A_{\pi_{\theta_k}}(s, a)-\nu A^C_{\pi_{\theta_k}}(s, a)\right)+\frac{\zeta}{\lambda}+1\right)

We chose :math:`\zeta` so that :math:`\sum_a \pi(a \mid s)=1` and
rewrite :math:`\zeta / \lambda+1` as :math:`Z_{\lambda, \nu}(s)`. We
find that the optimal solution :math:`\pi^*` to Equation
`[FOCOPS:lag1] <#FOCOPS:lag1>`__ takes the form

.. math:: \pi^*(a \mid s)=\frac{\pi_{\theta_k}(a \mid s)}{Z_{\lambda, \nu}(s)} \exp \left(\frac{1}{\lambda}\left(A_{\pi_{\theta_k}}(s, a)-\nu A^C_{\pi_{\theta_k}}(s, a)\right)\right)

Then we obtain:

.. math::

   \begin{aligned}
   &\underset{\substack{s \sim d^{\theta_{\theta_k}} \\
   a \sim \pi^*}}{\mathbb{E}}\left[A_{\pi_{\theta_k}}(s, a)-\nu A^C_{\pi_{\theta_k}}(s, a)-\lambda\left(\log \pi^*(a \mid s)-\log \pi_{\theta_k}(a \mid s)\right)\right] \\
   =&\underset{\substack{s \sim d^{\pi_{\theta_k}} \\
   a \sim \pi^*}}{\mathbb{E}}\left[A_{\pi_{\theta_k}}(s, a)-\nu A^C_{\pi_{\theta_k}}(s, a)-\lambda\left(\log \pi_{\theta_k}(a \mid s)-\log Z_{\lambda, \nu}(s)\right.\right. \\
   &\left.\left.+\frac{1}{\lambda}\left(A_{\pi_{\theta_k}}(s, a)-\nu A^C_{\pi_{\theta_k}}(s, a)\right)-\log \pi_{\theta_k}(a \mid s)\right)\right]\\
   =&\lambda\underset{\substack{s \sim d^{\theta_{\theta_k}} \\
   a \sim \pi^*}}{\mathbb{E}}[logZ_{\lambda,\nu}(s)]\nonumber
   \end{aligned}

Plugging the result back to Equation
`[FOCOPS:lag_dual] <#FOCOPS:lag_dual>`__, we obtain:

.. math::

   p^*=\underset{\lambda,\nu\ge0}{min}\lambda\delta+\nu\tilde{b}+\lambda\underset{\substack{s \sim d^{\theta_{\theta_k}} \\
   a \sim \pi^*}}{\mathbb{E}}[logZ_{\lambda,\nu}(s)]

Proof of Corollary `[FOCOPS:corollary1] <#FOCOPS:corollary1>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

*Proof.* We only need to calculate the gradient of the loss function for
a single sampled s. We first note that,

.. math::

   \begin{split}
   &D_{\mathrm{KL}}\left(\pi_\theta \| \pi^*\right)[s]\\ &=-\sum_a \pi_\theta(a \mid s) \log \pi^*(a \mid s)+\sum_a \pi_\theta(a \mid s) \log \pi_\theta(a \mid s) \\
   &=H\left(\pi_\theta, \pi^*\right)[s]-H\left(\pi_\theta\right)[s]
   \end{split}

where :math:`H\left(\pi_\theta\right)[s]` is the entropy and
:math:`H\left(\pi_\theta, \pi^*\right)[s]` is the cross-entropy under
state :math:`s`. The above equation is the basic mathematical knowledge
in information theory, which you can get in any information theory
textbook. We expand the cross entropy term which gives us:

.. math::

   \begin{aligned}
   &H\left(\pi_\theta, \pi^*\right)[s] =-\sum_a \pi_\theta(a \mid s) \log \pi^*(a \mid s) \\
   &=-\sum_a \pi_\theta(a \mid s) \log \left(\frac{\pi_{\theta_k}(a \mid s)}{Z_{\lambda, \nu}(s)} \exp \left[\frac{1}{\lambda}\left(A_{\pi_{\theta_k}}(s, a)-\nu A^C_{\pi_{\theta_k}}(s, a)\right)\right]\right) \\
   &=-\sum_a \pi_\theta(a \mid s) \log \pi_{\theta_k}(a \mid s)+\log Z_{\lambda, \nu}(s)\\&-\frac{1}{\lambda} \sum_a \pi_\theta(a \mid s)\left(A_{\pi_{\theta_k}}(s, a)-\nu A^C_{\pi_{\theta_k}}(s, a)\right)\nonumber
   \end{aligned}

We then subtract the entropy term to recover the KL divergence:

.. math::

   \begin{aligned}
   &D_{\mathrm{KL}}\left(\pi_\theta \| \pi^*\right)[s]=D_{\mathrm{KL}}\left(\pi_\theta \| \pi_{\theta_k}\right)[s]+\log Z_{\lambda, \nu}(s)-\\&\frac{1}{\lambda} \underset{a \sim \pi_{\theta_k}(\cdot \mid s)}{\mathbb{E}}\left[\frac{\pi_\theta(a \mid s)}{\pi_{\theta_k}(a \mid s)}\left(A_{\pi_{\theta_k}}(s, a)-\nu A^C_{\pi_{\theta_k}}(s, a)\right)\right]\nonumber
   \end{aligned}

where in the last equality we applied importance sampling to rewrite the
expectation w.r.t. :math:`\pi_{\theta_k}`. Finally, taking the gradient
on both sides gives us:

.. math::

   \begin{aligned}
   &\nabla_\theta D_{\mathrm{KL}}\left(\pi_\theta \| \pi^*\right)[s]=\nabla_\theta D_{\mathrm{KL}}\left(\pi_\theta \| \pi_{\theta_k}\right)[s]\\&-\frac{1}{\lambda} \underset{a \sim \pi_{\theta_k}(\cdot \mid s)}{\mathbb{E}}\left[\frac{\nabla_\theta \pi_\theta(a \mid s)}{\pi_{\theta_k}(a \mid s)}\left(A_{\pi_{\theta_k}}(s, a)-\nu A^C_{\pi_{\theta_k}}(s, a)\right)\right]\nonumber
   \end{aligned}

Proof of Corollary `[FOCOPS:corollary2] <#FOCOPS:corollary2>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

*Proof.* From Theorem
`[FOCOPS:Solving the optimal update policy] <#FOCOPS:Solving the optimal update policy>`__,
we have:

.. math:: L\left(\pi^*, \lambda, \nu\right)=\lambda \delta+\nu \tilde{b}+\lambda \underset{\substack{s \sim d^{\pi^*} \\ a \sim \pi^*}}{\mathbb{E}}\left[\log Z_{\lambda, \nu}(s)\right]

The first two terms is an affine function w.r.t. :math:`\nu`, therefore
its derivative is :math:`\tilde{b}`. We will then focus on the
expectation in the last term. To simplify our derivation, we will first
calculate the derivative of :math:`\pi^*` w.r.t. :math:`\nu`,

.. math::

   \begin{aligned}
   &\frac{\partial \pi^*(a \mid s)}{\partial \nu} \\=&\frac{\pi_{\theta_k}(a \mid s)}{Z_{\lambda, \nu}^2(s)}\left[Z_{\lambda, \nu}(s) \frac{\partial}{\partial \nu} \exp \left(\frac{1}{\lambda}\left(A_{\pi_{\theta_k}}(s, a)-\nu A^C_{\pi_{\theta_k}}(s, a)\right)\right)\right.\\
   &\left.-\exp \left(\frac{1}{\lambda}\left(A_{\pi_{\theta_k}}(s, a)-\nu A^C_{\pi_{\theta_k}}(s, a)\right)\right) \frac{\partial Z_{\lambda, \nu}(s)}{\partial \nu}\right] \\
   =&-\frac{A^C_{\pi_{\theta_k}}(s, a)}{\lambda} \pi^*(a \mid s)-\pi^*(a \mid s) \frac{\partial \log Z_{\lambda, \nu}(s)}{\partial \nu}\nonumber
   \end{aligned}

Therefore the derivative of the expectation in the last term of
:math:`L(\pi^*,\lambda,\nu)` can be written as:

.. math::

   \begin{aligned}\label{FOCOPS:proof_C2_1}
   & \frac{\partial}{\partial \nu} \underset{\substack{s \sim d^\pi \theta_k \\
   a \sim \pi^*}}{\mathbb{E}}\left[\log Z_{\lambda, \nu}(s)\right] \\
   =& \underset{\substack{s \sim d^{\pi_\theta} \\
   a \sim \pi_{\theta_k}}}{\mathbb{E}}\left[\frac{\partial}{\partial \nu}\left(\frac{\pi^*(a \mid s)}{\pi_{\theta_k}(a \mid s)} \log Z_{\lambda, \nu}(s)\right)\right] \\
   =& \underset{\substack{s \sim d^{\pi_\theta} \\
   a \sim \pi_{\theta_k}}}{\mathbb{E}}\left[\frac{1}{\pi_{\theta_k}(a \mid s)}\left(\frac{\partial \pi^*(a \mid s)}{\partial \nu} \log Z_{\lambda, \nu}(s)+\pi^*(a \mid s) \frac{\partial \log Z_{\lambda, \nu}(s)}{\partial \nu}\right)\right] \\
   =& \underset{\substack{s \sim d^{\pi_\theta} \\
   a \sim \pi^*}}{\mathbb{E}}\left[-\frac{A^C_{\pi_{\theta_k}}(s, a)}{\lambda} \log Z_{\lambda, \nu}(s)-\frac{\partial \log Z_{\lambda, \nu}(s)}{\partial \nu} \log Z_{\lambda, \nu}(s)+\frac{\partial \log Z_{\lambda, \nu}(s)}{\partial \nu}\right] .
   \end{aligned}

Also:

.. math::

   \begin{aligned}\label{FOCOPS:proof_C2_2}
   &\frac{\partial Z_{\lambda, \nu}(s)}{\partial \nu} \\
   &=\frac{\partial}{\partial \nu} \sum_a \pi_{\theta_k}(a \mid s) \exp \left(\frac{1}{\lambda}\left(A_{\pi_{\theta_k}}(s, a)-\nu A^C_{\pi_{\theta_k}}(s, a)\right)\right) \\
   &=\sum_a-\pi_{\theta_k}(a \mid s) \frac{A^C_{\pi_{\theta_k}}(s, a)}{\lambda} \exp \left(\frac{1}{\lambda}\left(A_{\pi_{\theta_k}}(s, a)-\nu A^C_{\pi_{\theta_k}}(s, a)\right)\right) \\
   &=\sum_a-\frac{A^C_{\pi_{\theta_k}}(s, a)}{\lambda} \frac{\pi_{\theta_k}(a \mid s)}{Z_{\lambda, \nu}(s)} \exp \left(\frac{1}{\lambda}\left(A_{\pi_{\theta_k}}(s, a)-\nu A^C_{\pi_{\theta_k}}(s, a)\right)\right) Z_{\lambda, \nu}(s) \\
   &=-\frac{Z_{\lambda, \nu}(s)}{\lambda} \underset{a \sim \pi^*(\cdot \mid s)}{\mathbb{E}}\left[A^C_{\pi_{\theta_k}}(s, a)\right] .
   \end{aligned}

Therefore:

.. math::

   \label{FOCOPS:proof_C2_3}
   \frac{\partial \log Z_{\lambda, \nu}(s)}{\partial \nu}=\frac{\partial Z_{\lambda, \nu}(s)}{\partial \nu} \frac{1}{Z_{\lambda, \nu}(s)}=-\frac{1}{\lambda} \underset{a \sim \pi^*(\cdot \mid s)}{\mathbb{E}}\left[A^C_{\pi_{\theta_k}}(s, a)\right] .

Plugging `[FOCOPS:proof_C2_3] <#FOCOPS:proof_C2_3>`__ into the last
equality in `[FOCOPS:proof_C2_1] <#FOCOPS:proof_C2_1>`__ gives us:

.. math::

   \label{FOCOPS:proof_C2_4}
   \begin{aligned}
   &\frac{\partial}{\partial \nu} \underset{\substack{s \sim d^{\pi_\theta} \\
   a \sim \pi^*}}{\mathbb{E}}\left[\log Z_{\lambda, \nu}(s)\right] \\
   &=\underset{\substack{s \sim d^{\pi^*} \\
   a \sim \pi^*}}{\mathbb{E}}\left[-\frac{A^C_{\pi_{\theta_k}}(s, a)}{\lambda} \log Z_{\lambda, \nu}(s)+\frac{A^C_{\pi_{\theta_k}}(s, a)}{\lambda} \log Z_{\lambda, \nu}(s)-\frac{1}{\lambda} A^C_{\pi_{\theta_k}}(s, a)\right] \\
   &=-\frac{1}{\lambda} \underset{\substack{s \sim d^{\pi_{\theta_k}} \\
   a \sim \pi^*}}{\mathbb{E}}\left[A^C_{\pi_{\theta_k}}(s, a)\right] .
   \end{aligned}

Combining `[FOCOPS:proof_C2_4] <#FOCOPS:proof_C2_4>`__ with the
derivatives of the affine term gives us the final desired result.