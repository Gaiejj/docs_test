Projection-Based Constrained Policy Optimization
================================================

Introduction
------------

| Projection-Based Constrained Policy
  Optimization :raw-latex:`\cite{pcpo}`
| (PCPO) is an iterative method for optimizing policy in a two-step
  process: the first step performs a local reward improvement update,
  while the second step reconciles any constraint violation by
  projecting the policy back onto the constraint set.

PCPO is an improvement work done on the basis of CPO
:raw-latex:`\cite{cpo}`. It provides a lower bound on reward
improvement, and an upper bound on constraint violation, for each policy
update just like CPO does. PCPO further characterizes the convergence of
PCPO based on two different metrics: L2 norm and Kullback-Leibler
divergence.

In a word, PCPO is a CPO-based algorithm dedicated to solving problem of
learning control policies that optimize a reward function while
satisfying constraints due to considerations of safety, fairness, or
other costs. If you have not previously learned the CPO type of
algorithm, in order to facilitate your complete understanding of the
PCPO algorithm ideas introduced in this section, we strongly recommend
that you read this article after reading the CPO tutorial we wrote.

Target
------

In the previous chapters, you learned that CPO solves the following
optimization problems:

.. math::

   
   \begin{aligned}
     & \pi_{k+1}=\arg\max_{\pi \in \Pi_{\theta}}J^R(\pi_{\theta})\\
     & s.t.\quad D(\pi,\pi_k)\le\delta\nonumber
     \\ & J^C\left(\pi_k\right)+\underset{\substack{s \sim d^{\pi_k} \\ a \sim \pi}}{\mathbb{E}}\left[A^C_{\pi_k}(s, a)\right] \leq d.\nonumber
   \end{aligned}

where :math:`\prod_{\theta}\subseteq\prod` denotes the set of
parametrized policies with parameters :math:`\theta`, and :math:`D` is
some distance measure. In local policy search for CMDPs, we additionally
require policy iterates to be feasible for the CMDP, so instead of
optimizing over :math:`\prod_{\theta}`, PCPO optimizes over
:math:`\prod_{\theta}\cap\prod_{C}`. Next, we will introduce you to how
PCPO solves the above optimization problems. In order for you to have a
clearer understanding, we hope that you will read the next section with
the following questions:

-  What is two-stage policy update and how?

-  What is performance bound for PCPO and how PCPO get it?

-  How PCPO practically solve the optimal problem?

Two-stage policy update
-----------------------

PCPO performs policy update in two stages. The first stage maximizes
reward using a trust region optimization method without constraints.
This might result in a new intermediate policy that does not satisfy the
constraints. The second stage reconciles the constraint violation (if
any) by projecting the policy back onto the constraint set, i.e.,
choosing the policy in the constraint set that is closest to the
selected intermediate policy. Next, we will describe how PCPO completes
the two-step update.

.. _`PCPO:Probelm1`:

Reward Improvement Step
~~~~~~~~~~~~~~~~~~~~~~~

First, PCPO optimizes the reward function by maximizing the reward
advantage function :math:`A_{\pi}(s,a)` subject to KL-Divergence
constraint. This constraints the intermediate policy
:math:`\pi^{k+\frac12}` to be within a :math:`\delta`-neighbourhood of
:math:`\pi_{k}`:

.. math::

   
   \begin{aligned}
     & \pi^{k+\frac12}=\underset{\pi}{\arg\,max}\underset{s\sim d^{\pi_k}, a\sim\pi}{\mathbb{E}}[A^R_{\pi_k}(s,a)]\\
     &  s.t.\, \underset{s\sim d^{\pi_k}}{\mathbb{E}}[D_{KL}(\pi||\pi_k)[s]]\le\delta\nonumber
   \end{aligned}

This update rule with the trust region is called Trust Region Policy
Optimization (TRPO) :raw-latex:`\cite{trpo}`. It constraints the policy
changes to a divergence neighborhood and guarantees reward improvement.

.. _`PCPO:Problem2`:

Projection step
~~~~~~~~~~~~~~~

Second, PCPO projects the intermediate policy :math:`\pi_{k+\frac12}`
onto the constraint set by minimizing a distance measure :math:`D`
between :math:`\pi_{k+\frac12}` and :math:`\pi`:

.. math::

   
   \begin{aligned}
       & \pi_{k+1}=\underset{\pi}{\arg\,min}\quad D(\pi,\pi_{k+\frac12})\\
       & s.t.\quad J^C\left(\pi_k\right)+\underset{\substack{s \sim d^{\pi_k} , a \sim \pi}}{\mathbb{E}}\left[A^C_{\pi_k}(s, a)\right] \leq d.\nonumber
   \end{aligned}

The projection step ensures that the constraint-satisfying policy
:math:`\pi_{k+1}` is close to :math:`\pi_{k+\frac12}`. Reward
improvement step ensures that the agent’s updates are in the direction
of maximizing rewards so as not to violate the step size of distance
measure :math:`D`. Projection step causes the agent to update in the
direction of satisfying the constraint while avoiding crossing :math:`D`
as much as possible.

Policy performance bounds
-------------------------

In safety-critical applications, how worse the performance of a system
evolves when applying a learning algorithm is an important issue. For
the two cases where the agent satisfies the constraint and does not
satisfy the constraint, PCPO provides worst-case performance bound
respectively.

.. container:: theorem

   Define
   :math:`\epsilon_{\pi_{k+1}}^{R}\doteq \max\limits_{s}\big|\mathbb{E}_{a\sim\pi_{k+1}}[A^{R}_{\pi_{k}}(s,a)]\big|`,
   and
   :math:`\epsilon_{\pi_{k+1}}^{C}\doteq \max\limits_{s}\big|\mathbb{E}_{a\sim\pi_{k+1}}[A^{C}_{\pi_{k}}(s,a)]\big|`.
   If the current policy :math:`\pi_k` satisfies the constraint, then
   under KL divergence projection, the lower bound on reward
   improvement, and upper bound on constraint violation for each policy
   update are

   .. math::

      J^{R}(\pi_{k+1})-J^{R}(\pi_{k})\geq-\frac{\sqrt{2\delta}\gamma\epsilon_{\pi_{k+1}}^{R}}{(1-\gamma)^{2}},
      %
      ~\text{and}~J^{C}(\pi_{k+1})\leq d+\frac{\sqrt{2\delta}\gamma\epsilon_{\pi_{k+1}}^{C}}{(1-\gamma)^{2}},

   where :math:`\delta` is the step size in the reward improvement step.

If you’ve read the CPO tutorial, then you should be familiar with the
above conclusions. In fact, a detailed proof of the above conclusions is
provided in the CPO tutorial.

.. container:: theorem

   Define
   :math:`\epsilon_{\pi_{k+1}}^{R}\doteq \max\limits_{s}\big|\mathbb{E}_{a\sim\pi_{k+1}}[A^{R}_{\pi_{k}}(s,a)]\big|`,
   :math:`\epsilon_{\pi_{k+1}}^{C}\doteq \max\limits_{s}\big|\mathbb{E}_{a\sim\pi_{k+1}}[A^{C}_{\pi_{k}}(s,a)]\big|`,
   :math:`b^{+}\doteq \max(0,J^{C}(\pi_k)-d),` and
   :math:`\alpha_{KL} \doteq \frac{1}{2a^T\boldsymbol{H}^{-1}a},` where
   :math:`a` is the gradient of the cost advantage function and
   :math:`\boldsymbol{H}` is the Hessian of the KL divergence
   constraint. If the current policy :math:`\pi_k` violates the
   constraint, then under KL divergence projection, the lower bound on
   reward improvement and the upper bound on constraint violation for
   each policy update are

   .. math::

      
      \begin{aligned}
      J^{R}(\pi_{k+1})-J^{R}(\pi_{k})\geq&-\frac{\sqrt{2(\delta+{b^+}^{2}\alpha_\mathrm{KL})}\gamma\epsilon_{\pi_{k+1}}^{R}}{(1-\gamma)^{2}}, \nonumber\\
      %
      ~\text{and}~J^{C}(\pi_{k+1})\leq& ~h+\frac{\sqrt{2(\delta+{b^+}^{2}\alpha_\mathrm{KL})}\gamma\epsilon_{\pi_{k+1}}^{C}}{(1-\gamma)^{2}},\nonumber
      \end{aligned}

   where :math:`\delta` is the step size in the reward improvement step.

The above theorem illustrates that when the policy has greater
constraint violation (:math:`b^+` increases), its worst-case performance
degradation increases. Note that Theorem 5.2 reduces to Theorem 5.1 if
the current policy :math:`\pi_k` satisfies the constraint (:math:`b^+` =
0). In the appendix a detailed proof of Theorem 5.2 is provided.

Practical Implementation
------------------------

For a large neural network policy with hundreds of thousands of
parameters, directly solving for the PCPO update in
`1.3.1 <#PCPO:Probelm1>`__ and `1.3.2 <#PCPO:Problem2>`__ is impractical
due to the computational cost. PCPO proposes that with a small step size
:math:`\delta`, the reward function and constraints and the KL
divergence constraint in the reward improvement step can be approximated
with a first order expansion, while the KL divergence measure in the
projection step can also be approximated with a second order expansion.

Reward improvement step
~~~~~~~~~~~~~~~~~~~~~~~

Define:

| :math:`g\doteq\nabla_\theta\underset{\substack{s\sim d^{\pi_k}a\sim \pi}}{\mathbb{E}}[A_{\pi_k}^{R}(s,a)]`
  is the gradient of the reward advantage function,
| :math:`a\doteq\nabla_\theta\underset{\substack{s\sim d^{\pi_k}a\sim \pi}}{\mathbb{E}}[A_{\pi_k}^{C}(s,a)]`
  is the gradient of the cost advantage function,

where
:math:`\boldsymbol{H}_{i,j}\doteq \frac{\partial^2 \underset{s\sim d^{\pi_{k}}}{\mathbb{E}}\big[KL(\pi ||\pi_{k})[s]\big]}{\partial \theta_j\partial \theta_j}`
is the Hessian of the KL divergence constraint (:math:`\boldsymbol{H}`
is also called the Fisher information matrix. It is symmetric positive
semi-definite), :math:`b\doteq J^{C}(\pi_k)-da` is the constraint
violation of the policy :math:`\pi_{k}`, and :math:`\theta` is the
parameter of the policy. PCPO linearizes the objective function at
:math:`\pi_k` subject to second order approximation of the KL divergence
constraint in order to obtain the following updates:

.. math::

   
   \begin{aligned}
       \theta_{k+\frac{1}{2}} = \underset{\theta}{\arg max}&\quad       g^{T}(\theta-\theta_k)  \nonumber\\
       \text{s.t.}&\quad\frac{1}{2}(\theta-\theta_{k})^{T}\boldsymbol{H}(\theta-\theta_k)\le \delta . \label{eq:update1}
   \end{aligned}

In fact, the above problem is essentially an optimization problem
presented in TRPO, which can be completely solved using the method we
introduced in the TRPO tutorial.

Projection step
~~~~~~~~~~~~~~~

PCPO provides a selection reference for distance measures: if the
projection is defined in the parameter space, L2 norm projection is
selected, while if the projection is defined in the probability space,
KL divergence projection is better. This can be approximated through the
second order expansion. Again, PCPO linearizes the cost constraint at
:math:`\pi_{k}.` This gives the following update for the projection
step:

.. math::

   
   \begin{aligned}
       \theta_{k+1} = \underset{\theta}{\arg min}&\quad \frac{1}{2}(\theta-{\theta}_{k+\frac{1}{2}})^{T}\boldsymbol{L}(\theta-{\theta}_{k+\frac{1}{2}}) \nonumber\\
       \text{s.t.}&\quad a^{T}(\theta-\theta_{k})+b\leq 0, \label{eq:update2}
   \end{aligned}

where :math:`\boldsymbol{L}=\boldsymbol{I}` for L2 norm projection, and
:math:`\boldsymbol{L}=\boldsymbol{H}` for KL divergence projection. PCPO
solves Problem (`[eq:update1] <#eq:update1>`__) and
Problem (`[eq:update2] <#eq:update2>`__) using convex programming (See
the appendix for the derivation). For each policy update:

.. math::

   
   \begin{aligned}
   \theta_{k+1}=\theta_{k}+&\sqrt{\frac{2\delta}{g^T\boldsymbol{H}^{-1}g}}\boldsymbol{H}^{-1}g
   -\max\left(0,\frac{\sqrt{\frac{2\delta}{g^T\boldsymbol{H}^{-1}g}}a^{T}\boldsymbol{H}^{-1}g+b}{a^T\boldsymbol{L}^{-1}a}\right)\boldsymbol{L}^{-1}a.
   \label{eq:PCPO_final}
   \end{aligned}

:math:`\boldsymbol{H}` is assumed invertible and PCPO requires to invert
:math:`\boldsymbol{H}`, which is impractical for huge neural network
policies. Hence it use the conjugate gradient method . (See appendix for
a discussion of the tradeoff between the approximation error and
computational efficiency of the conjugate gradient method.)

.. container:: tcolorbox

   | **Question:**\ Is using linear approximation to the constraint set
     enough to ensure constraint satisfaction since the real constraint
     set is maybe non-convex?
   | **Answer:**\ In fact, if the step size :math:`\delta` is small,
     then the linearization of the constraint set is accurate enough to
     locally approximate it.  
   |  
   | **Question:**\ Can PCPO solve the multi-constraint problem?
   | **Answer:**\ By sequentially projecting onto each of the sets, the
     update in Eq. (`[eq:PCPO_final] <#eq:PCPO_final>`__) can be
     extended by using alternating projections.

analysis
~~~~~~~~

The update rule in Eq. (`[eq:PCPO_final] <#eq:PCPO_final>`__) shows that
the difference between PCPO with KL divergence and L2 norm projections
is the cost update direction, leading to a difference in reward
improvement. These two projections converge to different stationary
points with different convergence rates related to the smallest and
largest singular values of the Fisher information matrix shown in
Theorem `[theorem:PCPO_converge] <#theorem:PCPO_converge>`__. PCPO
assumes that: PCPO *minimizes* the negative reward objective function
:math:`f: R^n \rightarrow R` . The function :math:`f` is
:math:`L`-smooth and twice continuously differentiable over the closed
and convex constraint set :math:`\mathcal{C}`.

.. container:: theorem

   Let
   :math:`\eta\doteq \sqrt{\frac{2\delta}{g^{T}\boldsymbol{H}^{-1}g}}`
   in Eq. (`[eq:PCPO_final] <#eq:PCPO_final>`__), where :math:`\delta`
   is the step size for reward improvement, :math:`g` is the gradient of
   :math:`f,` and :math:`\boldsymbol{H}` is the Fisher information
   matrix. Let :math:`\sigma_\mathrm{max}(\boldsymbol{H})` be the
   largest singular value of :math:`\boldsymbol{H},` and :math:`a` be
   the gradient of cost advantage function in
   Eq. (`[eq:PCPO_final] <#eq:PCPO_final>`__). Then PCPO with KL
   divergence projection converges to a stationary point either inside
   the constraint set or in the boundary of the constraint set. In the
   latter case, the Lagrangian constraint
   :math:`g=-\alpha a, \alpha\geq0` holds. Moreover, at step :math:`k+1`
   the objective value satisfies

   .. math:: f(\theta_{k+1})\leq f(\theta_{k})+||\theta_{k+1}-\theta_{k}||^2_{-\frac{1}{\eta}\boldsymbol{H}+\frac{L}{2}\boldsymbol{I}}.

   PCPO with L2 norm projection converges to a stationary point either
   inside the constraint set or in the boundary of the constraint set.
   In the latter case, the Lagrangian constraint
   :math:`\boldsymbol{H}^{-1}g=-\alpha a, \alpha\geq0` holds. If
   :math:`\sigma_\mathrm{max}(\boldsymbol{H})\leq1,` then a step
   :math:`k+1` objective value satisfies

   .. math:: f(\theta_{k+1})\leq f(\theta_{k})+(\frac{L}{2}-\frac{1}{\eta})||\theta_{k+1}-\theta_{k}||^2_2.

.. container:: proof

   *Proof.* See the appendix. ◻

Theorem `[theorem:PCPO_converge] <#theorem:PCPO_converge>`__ shows that
in the stationary point :math:`g` is a line that points to the opposite
direction of :math:`a.` Further, the improvement of the objective value
is affected by the singular value of the Fisher information matrix.
Specifically, the objective of KL divergence projection decreases when
:math:`\frac{L\eta}{2}\boldsymbol{I}\prec\boldsymbol{H},` implying that
:math:`\sigma_\mathrm{min}(\boldsymbol{H})> \frac{L\eta}{2}.` And the
objective of :math:`L2` norm projection decreases when
:math:`\eta<\frac{2}{L},` implying that condition number of
:math:`\boldsymbol{H}` is upper bounded:
:math:`\frac{\sigma_\mathrm{max}(\boldsymbol{H})}{\sigma_\mathrm{min}(\boldsymbol{H})}<\frac{2||g||^2_2}{L^2\delta}.`
Observing the singular values of the Fisher information matrix allows us
to adaptively choose the appropriate projection and hence achieve
objective improvement. In the supplemental material, we further use an
example to compare the optimization trajectories and stationary points
of KL divergence and L2 norm projections.

Appendix
--------

.. _`appendix:proof_theorem_1`:

Proof of Theorem `[theorem:feasibleCase] <#theorem:feasibleCase>`__: Performance Bound on Updating the Constraint-satisfying Policy
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In fact, we give a complete proof about Theorem
`[theorem:feasibleCase] <#theorem:feasibleCase>`__ in appendix
`[CPO:Performance bound] <#CPO:Performance bound>`__ of the CPO tutorial
. If you haven’t read it or forgotten it, please refer to it carefully
before reading the rest of the appendix

.. _`appendix:proof_theorem_2`:

Proof of Theorem `[theorem:infeasibleCase] <#theorem:infeasibleCase>`__: Performance Bound on Updating the Constraint-violating Policy
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To prove the policy performance bound when the current policy is
infeasible ( constraint-violating), we prove the KL divergence between
:math:`\pi_{k}` and :math:`\pi^{k+1}` for the KL divergence projection.
We then prove the main theorem for the worst-case performance
degradation.

.. container:: lemma

   If the current policy :math:`\pi_{k}` satisfies the constraint, the
   constraint set is closed and convex, the KL divergence constraint for
   the first step is
   :math:`\mathbb{E}_{s\sim d^{\pi_{k}}}\big[\mathrm{KL}(\pi^{k+\frac{1}{2}} ||\pi_{k})[s]\big]\leq \delta,`
   where :math:`\delta` is the step size in the reward improvement step,
   then under KL divergence projection, we have

   .. math:: \mathbb{E}_{s\sim d^{\pi_{k}}}\big[\mathrm{KL}(\pi^{k+1} ||\pi_{k})[s]\big]\leq \delta.

.. container:: proof

   *Proof.* By the Bregman divergence projection inequality,
   :math:`\pi_{k}` being in the constraint set, and :math:`\pi^{k+1}`
   being the projection of the :math:`\pi^{k+\frac{1}{2}}` onto the
   constraint set, we have

   .. math::

      
      \begin{aligned}
      &\mathbb{E}_{s\sim d^{\pi_{k}}}\big[\mathrm{KL}(\pi_{k} ||\pi_{k+\frac{1}{2}})[s]\big]\geq 
      %
      \mathbb{E}_{s\sim d^{\pi_{k}}}\big[\mathrm{KL}(\pi_{k}||\pi_{k+1})[s]\big] \\
      &+
      %
      \mathbb{E}_{s\sim d^{\pi_{k}}}\big[\mathrm{KL}(\pi_{k+1} ||\pi_{k+\frac{1}{2}})[s]\big]\\
      %
      &\Rightarrow\delta\geq
      %
      \mathbb{E}_{s\sim d^{\pi_{k}}}\big[\mathrm{KL}(\pi_{k} ||\pi_{k+\frac{1}{2}})[s]\big]\geq 
      %
      \mathbb{E}_{s\sim d^{\pi_{k}}}\big[\mathrm{KL}(\pi_{k}||\pi_{k+1})[s]\big].
      \end{aligned}

   The derivation uses the fact that KL divergence is always greater
   than zero. We know that KL divergence is asymptotically symmetric
   when updating the policy within a local neighbourhood. Thus, we have

   .. math::
      
      \delta\geq
      %
      \mathbb{E}_{s\sim d^{\pi_{k}}}\big[\mathrm{KL}(\pi_{k+\frac{1}{2}} ||\pi_{k})[s]\big]\geq 
      %
      \mathbb{E}_{s\sim d^{\pi_{k}}}\big[\mathrm{KL}(\pi_{k+1}||\pi_{k})[s]\big].

    ◻

.. container:: lemma

   If the current policy :math:`\pi_{k}` violates the constraint, the
   constraint set is closed and convex, the KL divergence constraint for
   the first step is
   :math:`\mathbb{E}_{s\sim d^{\pi_{k}}}\big[\mathrm{KL}(\pi_{k+\frac{1}{2}} ||\pi_{k})[s]\big]\leq \delta,`
   where :math:`\delta` is the step size in the reward improvement step,
   then under the KL divergence projection, we have

   .. math:: \mathbb{E}_{s\sim d^{\pi_{k}}}\big[\mathrm{KL}(\pi_{k+1} ||\pi_{k})[s]\big]\leq \delta+{b^+}^2\alpha_\mathrm{KL},

   where
   :math:`\alpha_\mathrm{KL} \doteq \frac{1}{2a^T\boldsymbol{H}^{-1}a},`
   :math:`a` is the gradient of the cost advantage function,

   :math:`\boldsymbol{H}` is the Hessian of the KL divergence
   constraint, and :math:`b^+\doteq\max(0,J^{C}(\pi_k)-h).`

.. container:: proof

   *Proof.* We define the sublevel set of cost constraint function for
   the current infeasible policy :math:`\pi_k`:

   .. math:: L^{\pi_k}=\{\pi~|~J^{C}(\pi_{k})+ \mathbb{E}_{\substack{s\sim d^{\pi_{k}}\\ a\sim \pi}}[A_{\pi_k}^{C}(s,a)]\leq J^{C}(\pi_{k})\}.

   This implies that the current policy :math:`\pi_k` lies in
   :math:`L^{\pi_k}`, and :math:`\pi^{k+\frac{1}{2}}` is projected onto
   the constraint set:
   :math:`\{\pi~|~J^{C}(\pi_{k})+ \mathbb{E}_{\substack{s\sim d^{\pi_{k}}\\ a\sim \pi}}[A_{\pi_k}^{C}(s,a)]\leq h\}.`
   Next, we define the policy :math:`\pi_{k+1}^l` as the projection of
   :math:`\pi_{k+\frac{1}{2}}` onto :math:`L^{\pi_k}.`

   By the Three-point Lemma, for these three polices
   :math:`\pi_k, \pi_{k+1},` and :math:`\pi^{k+1}_l`, with
   :math:`\varphi(x)\doteq\sum_i x_i\log x_i`, we have

   .. math::

      
      \begin{aligned}
      \delta \geq  \mathbb{E}_{s\sim d^{\pi_{k}}}\big[\mathrm{KL}(\pi_{k+1}^l ||\pi_{k})[s]\big]&=\mathbb{E}_{s\sim d^{\pi_{k}}}\big[\mathrm{KL}(\pi_{k+1} ||\pi_{k})[s]\big] \nonumber\\ 
      &-\mathbb{E}_{s\sim d^{\pi_{k}}}\big[\mathrm{KL} (\pi_{k+1} ||\pi_{k+1}^l)[s]\big] \nonumber \\
      &+\mathbb{E}_{s\sim d^{\pi_{k}}}\big[(\nabla\varphi(\pi_k)-\nabla\varphi(\pi_{k+1}^{l}))^T(\pi_{k+1}-\pi_{k+1}^l)[s]\big] \nonumber \\
      \Rightarrow \mathbb{E}_{s\sim d^{\pi_{k}}}\big[\mathrm{KL} (\pi_{k+1} ||\pi_{k})[s]\big]&\leq \delta + \mathbb{E}_{s\sim d^{\pi_{k}}}\big[\mathrm{KL} (\pi_{k+1} ||\pi_{k+1}^l)[s]\big] \nonumber \\
      &- \mathbb{E}_{s\sim d^{\pi_{k}}}\big[(\nabla\varphi(\pi_k)-\nabla\varphi(\pi_{k+1}^{l}))^T(\pi^{k+1}-\pi^{k+1}_l)[s]\big]. \label{eq:infeasible_firstPart}
      \end{aligned}

   The inequality
   :math:`\mathbb{E}_{s\sim d^{\pi_{k}}}\big[\mathrm{KL} (\pi^{k+1}_l ||\pi_{k})[s]\big]\leq\delta`
   comes from that :math:`\pi_{k}` and :math:`\pi^{k+1}_l` are in
   :math:`L^{\pi_k}`, and Lemma `[lemma:feasible] <#lemma:feasible>`__.

   If the constraint violation of the current policy :math:`\pi_k` is
   small, :math:`b^+` is small,
   :math:`\mathbb{E}_{s\sim d^{\pi_{k}}}\big[\mathrm{KL} (\pi^{k+1} ||\pi^{k+1}_l)[s]\big]`
   can be approximated by the second order expansion. By the update rule
   in Eq. (`[eq:PCPO_final] <#eq:PCPO_final>`__), we have

   .. math::
      
      \begin{aligned}
      \mathbb{E}_{s\sim d^{\pi_{k}}}\big[\mathrm{KL}(\pi^{k+1} ||\pi^{k+1}_l)[s]\big] &\approx \frac{1}{2}(\theta_{k+1}-\theta_{k+1}^l)^{T}\boldsymbol{H}(\theta_{k+1}-\theta_{k+1}^l)\nonumber\\
      &=\frac{1}{2} \Big(\frac{b^+}{a^T\boldsymbol{H}^{-1}a}\boldsymbol{H}^{-1}a\Big)^T\boldsymbol{H}\Big(\frac{b^+}{a^T\boldsymbol{H}^{-1}a}\boldsymbol{H}^{-1}a\Big)\nonumber\\
      &=\frac{{b^+}^2}{2a^T\boldsymbol{H}^{-1}a}\nonumber\\
      &={b^+}^2\alpha_\mathrm{KL}, \label{eq:infeasible_secondPart}
      \end{aligned}

   where
   :math:`\alpha_\mathrm{KL} \doteq \frac{1}{2a^T\boldsymbol{H}^{-1}a}.`

   And since :math:`\delta` is small, we have
   :math:`\nabla\varphi(\pi_k)-\nabla\varphi(\pi_{k+1}^{l})\approx \mathbf{0}`
   given :math:`s`. Thus, the third term in
   Eq. (`[eq:infeasible_firstPart] <#eq:infeasible_firstPart>`__) can be
   eliminated.

   Combining
   Eq. (`[eq:infeasible_firstPart] <#eq:infeasible_firstPart>`__) and
   Eq. (`[eq:infeasible_secondPart] <#eq:infeasible_secondPart>`__), we
   have :math:`[
   \mathbb{E}_{s\sim d^{\pi_{k}}}\big[\mathrm{KL}(\pi^{k+1}||\pi_{k})[s]\big]\leq \delta+{b^+}^2\alpha_\mathrm{KL}.]` ◻

Now we use Lemma `[lemma:infeasible] <#lemma:infeasible>`__ to prove the
main theorem.

.. container:: theorem

   Define
   :math:`\epsilon_{\pi_{k+1}}^{R}\doteq \max\limits_{s}\big|\mathbb{E}_{a\sim\pi^{k+1}}[A_{R}^{\pi_{k}}(s,a)]\big|`,
   :math:`\epsilon_{\pi_{k+1}}^{C}\doteq \max\limits_{s}\big|\mathbb{E}_{a\sim\pi^{k+1}}[A_{C}^{\pi_{k}}(s,a)]\big|`,
   :math:`b^{+}\doteq \max(0,J^{C}(\pi_k)-h),` and
   :math:`\alpha_\mathrm{KL} \doteq \frac{1}{2a^T\boldsymbol{H}^{-1}a},`
   where :math:`a` is the gradient of the cost advantage function and
   :math:`\boldsymbol{H}` is the Hessian of the KL divergence
   constraint. If the current policy :math:`\pi_k` violates the
   constraint, then under the KL divergence projection, the lower bound
   on reward improvement and the upper bound on constraint violation for
   each policy update are

   .. math::

      
      \begin{aligned}
      J^{R}(\pi_{k+1})-J^{R}(\pi_{k})\geq&-\frac{\sqrt{2(\delta+{b^+}^{2}\alpha_\mathrm{KL})}\gamma\epsilon_{\pi_{k+1}}^{R}}{(1-\gamma)^{2}}, \nonumber\\
      %
      ~\text{and}~J^{C}(\pi^{k+1})\leq&~h+\frac{\sqrt{2(\delta+{b^+}^{2}\alpha_\mathrm{KL})}\gamma\epsilon_{\pi_{k+1}}^{C}}{(1-\gamma)^{2}},\nonumber
      \end{aligned}

   where :math:`\delta` is the step size in the reward improvement step.

.. container:: proof

   *Proof.* Following the same proof in Theorem
   `[theorem:feasibleCase] <#theorem:feasibleCase>`__, we complete the
   proof. ◻

Note that the bounds we obtain for the infeasibe case.

.. _`appendix:proof_update_rule_1`:

Proof of Analytical Solution to PCPO
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. container:: theorem

   Consider the PCPO problem. In the first step, we optimize the reward:

   .. math::
      
      \begin{aligned}
          \theta_{k+\frac{1}{2}} = \underset{\theta}{\arg\,min}&\quad g^{T}(\theta-\theta_{k}) \\
          \text{s.t.}&\quad\frac{1}{2}(\theta-\theta_{k})^{T}\boldsymbol{H}(\theta-\theta_{k})\leq \delta,
      \end{aligned}

   and in the second step, we project the policy onto the constraint
   set:

   .. math::
      
      \begin{aligned}
          \theta_{k+1} = \underset{\theta}{\arg\,min}\quad \frac{1}{2}(\theta-{\theta}_{k+\frac{1}{2}})^{T}\boldsymbol{L}(\theta-{\theta}_{k+\frac{1}{2}}) \\
          \text{s.t.}&\quad a^{T}(\theta-\theta_{k})+b\leq 0,
      \end{aligned}

   where :math:`g, a, \theta \in R^n, b, \delta\in R, \delta>0,` and
   :math:`\boldsymbol{H},\boldsymbol{L}\in R^{n\times n}, \boldsymbol{L}=\boldsymbol{H}`
   if using the KL divergence projection, and
   :math:`\boldsymbol{L}=\boldsymbol{I}` if using the L2 norm
   projection. When there is at least one strictly feasible point, the
   optimal solution satisfies

   .. math::
      
      \theta_{k+1}=\theta_{k}+\sqrt{\frac{2\delta}{g^T\boldsymbol{H}^{-1}g}}\boldsymbol{H}^{-1}g\nonumber\\
      -\max(0,\frac{\sqrt{\frac{2\delta}{g^T\boldsymbol{H}^{-1}g}}a^{T}\boldsymbol{H}^{-1}g+b}{a^T\boldsymbol{L}^{-1}a})\boldsymbol{L}^{-1}a,

   assuming that :math:`\boldsymbol{H}` is invertible to get a unique
   solution.

.. container:: proof

   *Proof.* For the first problem, since :math:`\boldsymbol{H}` is the
   Fisher Information matrix, which automatically guarantees it is
   positive semi-definite. Hence it is a convex program with quadratic
   inequality constraints. Hence if the primal problem has a feasible
   point, then Slater’s condition is satisfied and strong duality holds.
   Let :math:`\theta^{*}` and :math:`\lambda^*` denote the solutions to
   the primal and dual problems, respectively. In addition, the primal
   objective function is continuously differentiable. Hence the
   Karush-Kuhn-Tucker (KKT) conditions are necessary and sufficient for
   the optimality of :math:`\theta^{*}` and :math:`\lambda^*.` We now
   form the Lagrangian:

   .. math:: \mathcal{L}(\theta,\lambda)=-g^{T}(\theta-\theta_{k})+\lambda\Big(\frac{1}{2}(\theta-\theta_{k})^{T}\boldsymbol{H}(\theta-\theta_{k})- \delta\Big).

   And we have the following KKT conditions:

   .. math::
      
      \begin{aligned}
         -g + \lambda^*\boldsymbol{H}\theta^{*}-\lambda^*\boldsymbol{H}\theta_{k}=0~~~~&~~~\nabla_\theta\mathcal{L}(\theta^{*},\lambda^{*})=0 \label{KKT_1}\\
         \frac{1}{2}(\theta^{*}-\theta_{k})^{T}\boldsymbol{H}(\theta^{*}-\theta_{k})- \delta=0~~~~&~~~\nabla_\lambda\mathcal{L}(\theta^{*},\lambda^{*})=0 \label{KKT_2}\\
          \frac{1}{2}(\theta^{*}-\theta_{k})^{T}\boldsymbol{H}(\theta^{*}-\theta_{k})-\delta\leq0~~~~&~~~\text{primal constraints}\label{KKT_3}\\
         \lambda^*\geq0~~~~&~~~\text{dual constraints}\label{KKT_4}\\
         \lambda^*\Big(\frac{1}{2}(\theta^{*}-\theta_{k})^{T}\boldsymbol{H}(\theta^{*}-\theta_{k})-\delta\Big)=0~~~~&~~~\text{complementary slackness}\label{KKT_5}
      \end{aligned}

   By Eq. (`[KKT_1] <#KKT_1>`__), we have
   :math:`\theta^{*}=\theta_{k}+\frac{1}{\lambda^*}\boldsymbol{H}^{-1}g.`
   And by plugging Eq. (`[KKT_1] <#KKT_1>`__) into
   Eq. (`[KKT_2] <#KKT_2>`__), we have
   :math:`\lambda^*=\sqrt{\frac{g^T\boldsymbol{H}^{-1}g}{2\delta}}.`
   Hence we have our optimal solution:

   .. math::
      
      \begin{aligned}
      \theta_{k+\frac{1}{2}}=\theta^{*}=\theta_{k}+\sqrt{\frac{2\delta}{g^T\boldsymbol{H}^{-1}g}}\boldsymbol{H}^{-1}g, \label{KKT_First}    
      \end{aligned}

   which also satisfies Eq. (`[KKT_3] <#KKT_3>`__),
   Eq. (`[KKT_4] <#KKT_4>`__), and Eq. (`[KKT_5] <#KKT_5>`__).

   Following the same reasoning, we now form the Lagrangian of the
   second problem:

   .. math::
      
      \begin{aligned}
      \mathcal{L}(\theta,\lambda)=\frac{1}{2}(\theta-{\theta}_{k+\frac{1}{2}})^{T}\boldsymbol{L}(\theta-{\theta}_{k+\frac{1}{2}})+\lambda(a^T(\theta-\theta_{k})+b). \nonumber
      \end{aligned}

   And we have the following KKT conditions:

   .. math::
      
      \begin{aligned}
        \boldsymbol{L}\theta^*-\boldsymbol{L}\theta_{k+\frac{1}{2}}+\lambda^*a=0~~~~&~~~\nabla_\theta\mathcal{L}(\theta^{*},\lambda^{*})=0 \label{KKT_6}\\
         a^T(\theta^*-\theta_{k})+b=0~~~~&~~~\nabla_\lambda\mathcal{L}(\theta^{*},\lambda^{*})=0 \label{KKT_7}\\
          a^T(\theta^*-\theta_{k})+b\leq0~~~~&~~~\text{primal constraints}\label{KKT_8}\\
         \lambda^*\geq0~~~~&~~~\text{dual constraints}\label{KKT_9}\\
         \lambda^*(a^T(\theta^*-\theta_{k})+b)=0~~~~&~~~\text{complementary slackness}\label{KKT_10}
      \end{aligned}

   By Eq. (`[KKT_6] <#KKT_6>`__), we have
   :math:`\theta^{*}=\theta_{k+1}+\lambda^*\boldsymbol{L}^{-1}a.` And by
   plugging Eq. (`[KKT_6] <#KKT_6>`__) into Eq. (`[KKT_7] <#KKT_7>`__)
   and Eq. (`[KKT_9] <#KKT_9>`__), we have
   :math:`\lambda^*=\max(0,\\ \frac{a^T(\theta_{k+\frac{1}{2}}-\theta_{k})+b}{a\boldsymbol{L}^{-1}a}).`
   Hence we have our optimal solution:

   .. math::
      
      \begin{aligned}
      \theta_{k+1}=\theta^{*}=\theta_{k+\frac{1}{2}}-\max(0,\frac{a^T(\theta_{k+\frac{1}{2}}-\theta_{k})+b}{a^T\boldsymbol{L}^{-1}a^T})\boldsymbol{L}^{-1}a,\label{KKT_Second}
      \end{aligned}

   which also satisfies Eq. (`[KKT_8] <#KKT_8>`__) and
   Eq. (`[KKT_10] <#KKT_10>`__). Hence by
   Eq. (`[KKT_First] <#KKT_First>`__) and
   Eq. (`[KKT_Second] <#KKT_Second>`__), we have

   .. math::

      \theta_{k+1}=\theta_{k}+\sqrt{\frac{2\delta}{g^T\boldsymbol{H}^{-1}g}}\boldsymbol{H}^{-1}g\nonumber\\
      -\max(0,\frac{\sqrt{\frac{2\delta}{g^T\boldsymbol{H}^{-1}g}}a^{T}\boldsymbol{H}^{-1}g+b}{a^T\boldsymbol{L}^{-1}a})\boldsymbol{L}^{-1}a.

    ◻

.. _`appendix:proof_theorem_3`:

Proof of Theorem `[theorem:PCPO_converge] <#theorem:PCPO_converge>`__: Stationary Points of PCPO with the KL divergence and L2 Norm Projections
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For our analysis, we make the following assumptions: we *minimize* the
negative reward objective function :math:`f: R^n \rightarrow R` (We
follow the convention of the literature that authors typically minimize
the objective function). The function :math:`f` is :math:`L`-smooth and
twice continuously differentiable over the closed and convex constraint
set :math:`\mathcal{C}.` We have the following lemma to characterize the
projection and for the proof of Theorem
`[theorem:PCPO_converge_appendix] <#theorem:PCPO_converge_appendix>`__.

.. container:: lemma

   For any :math:`\theta,`
   :math:`\theta^{*}=\mathrm{Proj}^{\boldsymbol{L}}_{\mathcal{C}}(\theta)`
   if and only if
   :math:`(\theta-\theta^*)^T\boldsymbol{L}(\theta'-\theta^*)\leq0, \forall\theta'\in\mathcal{C},`
   where
   :math:`\mathrm{Proj}^{\boldsymbol{L}}_{\mathcal{C}}(\theta)\doteq \underset{\theta' \in \mathrm{C}}{\arg\,min}||\theta-\theta'||^2_{\boldsymbol{L}}`
   and :math:`\boldsymbol{L}=\boldsymbol{H}` if using the KL divergence
   projection, and :math:`\boldsymbol{L}=\boldsymbol{I}` if using the L2
   norm projection.

.. container:: proof

   *Proof.* :math:`(\Rightarrow)` Let
   :math:`\theta^{*}=\mathrm{Proj}^{\boldsymbol{L}}_{\mathcal{C}}(\theta)`
   for a given :math:`\theta \not\in\mathcal{C},`
   :math:`\theta'\in\mathcal{C}` be such that
   :math:`\theta'\neq\theta^*,` and :math:`\alpha\in(0,1).` Then we have

   .. math::
      
      \begin{split}\label{eq:appendix_lemmaD1_0}
      \left\|\theta-\theta^*\right\|_L^2
      &\leq\left\|\theta-\left(\theta^*+\alpha\left(\theta^{\prime}-\theta^*\right)\right)\right\|_L^2 \\
      &=\left\|\theta-\theta^*\right\|_L^2+\alpha^2\left\|\theta^{\prime}-\theta^*\right\|_{\boldsymbol{L}}^2\\
      &~~~~ -2\alpha\left(\theta-\theta^*\right)^T \boldsymbol{L}\left(\theta^{\prime}-\theta^*\right) \\
      & \Rightarrow\left(\theta-\theta^*\right)^T \boldsymbol{L}\left(\theta^{\prime}-\theta^*\right) \leq \frac{\alpha}{2}\left\|\theta^{\prime}-\theta^*\right\|_{\boldsymbol{L}}^2
      \end{split}

   Since the right hand side of Eq.
   (`[eq:appendix_lemmaD1_0] <#eq:appendix_lemmaD1_0>`__) can be made
   arbitrarily small for a given :math:`\alpha`, and hence we have:

   .. math:: (\theta-\theta^*)^T\boldsymbol{L}(\theta'-\theta^*)\leq0, \forall\theta'\in\mathcal{C}.

   Let :math:`\theta^*\in\mathcal{C}` be such that
   :math:`(\theta-\theta^*)^T\boldsymbol{L}(\theta'-\theta^*)\leq0, \forall\theta'\in\mathcal{C}.`
   We show that :math:`\theta^*` must be the optimal solution. Let
   :math:`\theta'\in\mathcal{C}` and :math:`\theta'\neq\theta^*.` Then
   we have

   .. math::
      
      \begin{split}
      &\left\|\theta-\theta^{\prime}\right\|_L^2-\left\|\theta-\theta^*\right\|_L^2\\ &=\left\|\theta-\theta^*+\theta^*-\theta^{\prime}\right\|_L^2-\left\|\theta-\theta^*\right\|_L^2 \\
      &=\left\|\theta-\theta^*\right\|_L^2+\left\|\theta^{\prime}-\theta^*\right\|_L^2-2\left(\theta-\theta^*\right)^T \boldsymbol{L}\left(\theta^{\prime}-\theta^*\right)\\
      &~~~~-\left\|\theta-\theta^*\right\|_{\boldsymbol{L}}^2 \\
      &>0 \\
      &\Rightarrow\left\|\theta-\theta^{\prime}\right\|_L^2 >\left\|\theta-\theta^*\right\|_L^2 .
      \end{split}

   Hence, :math:`\theta^*` is the optimal solution to the optimization
   problem, and
   :math:`\theta^*=\mathrm{Proj}^{\boldsymbol{L}}_{\mathcal{C}}(\theta).` ◻

Based on Lemma
`[lemma:projecion_appendix] <#lemma:projecion_appendix>`__, we have the
following theorem.

.. container:: theorem

   Let :math:`\eta\doteq \sqrt{\frac{2\delta}{g^{T}\boldsymbol{H}^{-1}g}}` in
   Eq. (`[eq:PCPO_final] <#eq:PCPO_final>`__), where :math:`\delta` is
   the step size for reward improvement, :math:`g` is the gradient of
   :math:`f,` :math:`\boldsymbol{H}` is the Fisher information matrix. Let
   :math:`\sigma_\mathrm{max}(\boldsymbol{H})` be the largest singular value of
   :math:`\boldsymbol{H},` and :math:`a` be the gradient of cost advantage
   function in Eq. (`[eq:PCPO_final] <#eq:PCPO_final>`__). Then PCPO
   with the KL divergence projection converges to stationary points with
   :math:`g\in-a` (i.e., the gradient of :math:`f` belongs to the
   negative gradient of the cost advantage function). The objective
   value changes by

   .. math::
      
      \begin{aligned}
      f(\theta_{k+1})\leq f(\theta_{k})+||\theta_{k+1}-\theta_{k}||^2_{-\frac{1}{\eta}\boldsymbol{H}+\frac{L}{2}\boldsymbol{I}}.\label{eq:bound1}  
      \end{aligned}

   PCPO with the L2 norm projection converges to stationary points with
   :math:`\boldsymbol{H}^{-1}g\in-a` (i.e., the product of the inverse
   of :math:`\boldsymbol{H}` and gradient of :math:`f` belongs to the
   negative gradient of the cost advantage function). If
   :math:`\sigma_\mathrm{max}(\boldsymbol{H})\leq1,` then the objective
   value changes by

   .. math::
      
      \begin{aligned}
      f(\theta_{k+1})\leq f(\theta_{k})+(\frac{L}{2}-\frac{1}{\eta})||\theta_{k+1}-\theta_{k}||^2_2.\label{eq:bound2}  
      \end{aligned}

.. container:: proof

   *Proof.* The proof of the theorem is based on working in a Hilbert
   space and the non-expansive property of the projection. We first
   prove stationary points for PCPO with the KL divergence and L2 norm
   projections, and then prove the change of the objective value.

   When in stationary points :math:`\theta^*`, we have

   .. math::
      
      \begin{aligned}
      &\theta^{*}\\
      &=\theta^{*}-\sqrt{\frac{2\delta}{g^T\boldsymbol{H}^{-1}g}}\boldsymbol{H}^{-1}g
      -\max\left(0,\frac{\sqrt{\frac{2\delta}{g^T\boldsymbol{H}^{-1}g}}a^{T}\boldsymbol{H}^{-1}g+b}{a^T\boldsymbol{L}^{-1}a}\right)\boldsymbol{L}^{-1}a. \nonumber\\
      \Leftrightarrow &\sqrt{\frac{2\delta}{g^T\boldsymbol{H}^{-1}g}}\boldsymbol{H}^{-1}g  = -\max(0,\frac{\sqrt{\frac{2\delta}{g^T\boldsymbol{H}^{-1}g}}a^{T}\boldsymbol{H}^{-1}g+b}{a^T\boldsymbol{L}^{-1}a})\boldsymbol{L}^{-1}a \nonumber\\
      \Leftrightarrow & \boldsymbol{H}^{-1}g \in -\boldsymbol{L}^{-1}a.
      \label{eq:appendixStationary}
      \end{aligned}

   For the KL divergence projection (:math:`\boldsymbol{L}=\boldsymbol{H}`),
   Eq. (`[eq:appendixStationary] <#eq:appendixStationary>`__) boils down
   to :math:`g\in-a,` and for the L2 norm projection
   (:math:`\boldsymbol{L}=\boldsymbol{I}`),
   Eq. (`[eq:appendixStationary] <#eq:appendixStationary>`__) is
   equivalent to :math:`\boldsymbol{H}^{-1}g\in-a.`

   Now we prove the second part of the theorem. Based on Lemma
   `[lemma:projecion_appendix] <#lemma:projecion_appendix>`__, for the
   KL divergence projection, we have

   .. math::
      
      \begin{gathered}
      \label{eq:appendix_converge_0}
      \left(\theta_k-\theta_{k+1}\right)^T \boldsymbol{H}\left(\theta_k-\eta \boldsymbol{H}^{-1} \boldsymbol{g}-\theta_{k+1}\right) \leq 0 \\
      \Rightarrow \boldsymbol{g}^T\left(\theta_{k+1}-\theta_k\right) \leq-\frac{1}{\eta}\left\|\theta_{k+1}-\theta_k\right\|_{\boldsymbol{H}}^2
      \end{gathered}

   By Eq. (`[eq:appendix_converge_0] <#eq:appendix_converge_0>`__), and
   :math:`L`-smooth continuous function :math:`f,` we have

   .. math::
      
      \begin{aligned}
      f\left(\theta_{k+1}\right) & \leq f\left(\theta_k\right)+\boldsymbol{g}^T\left(\theta_{k+1}-\theta_k\right)+\frac{L}{2}\left\|\theta_{k+1}-\theta_k\right\|_2^2 \\
      & \leq f\left(\theta_k\right)-\frac{1}{\eta}\left\|\theta_{k+1}-\theta_k\right\|_{\boldsymbol{H}}^2+\frac{L}{2}\left\|\theta_{k+1}-\theta_k\right\|_2^2 \\
      &=f\left(\theta_k\right)+\left(\theta_{k+1}-\theta_k\right)^T\left(-\frac{1}{\eta} \boldsymbol{H}+\frac{L}{2} \boldsymbol{I}\right)\left(\theta_{k+1}-\theta_k\right) \\
      &=f\left(\theta_k\right)+\left\|\theta_{k+1}-\theta_k\right\|_{-\frac{1}{\eta} \boldsymbol{H}+\frac{L}{2} \boldsymbol{I}}^2
      \end{aligned}

   For the L2 norm projection, we have

   .. math::
      
      \begin{aligned}
      \label{eq:appendix_converge_1}
         (\theta_{k}-\theta_{k+1})^T(\theta_{k}-\eta\boldsymbol{H}^{-1}g-\theta_{k+1})\leq0\nonumber\\
         \Rightarrow g^T\boldsymbol{H}^{-1}(\theta_{k+1}-\theta_{k})\leq -\frac{1}{\eta}||\theta_{k+1}-\theta_{k}||^2_2.
      \end{aligned}

   By Eq. (`[eq:appendix_converge_1] <#eq:appendix_converge_1>`__),
   :math:`L`-smooth continuous function :math:`f,` and if
   :math:`\sigma_\mathrm{max}(\boldsymbol{H})\leq1,` we have

   .. math::

      \begin{aligned}
          f(\theta_{k+1})&\leq f(\theta_{k})+g^T(\theta_{k+1}-\theta_{k})+\frac{L}{2}||\theta_{k+1}-\theta_{k}||^2_2 \nonumber\\
          &\leq f(\theta_{k})+(\frac{L}{2}-\frac{1}{\eta})||\theta_{k+1}-\theta_{k}||^2_2.\nonumber
      \end{aligned}

   To see why we need the assumption of
   :math:`\sigma_\mathrm{max}(\boldsymbol{H})\leq1,` we define
   :math:`\boldsymbol{H}=\boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{U}^T`
   as the singular value decomposition of :math:`\boldsymbol{H}` with
   :math:`u_i` being the column vector of :math:`\boldsymbol{U}.` Then
   we have

   .. math::

      \begin{aligned}
          g^T\boldsymbol{H}^{-1}(\theta_{k+1}-\theta_{k})
          &=g^T\boldsymbol{U}\boldsymbol{\Sigma}^{-1}\boldsymbol{U}^T(\theta_{k+1}-\theta_{k}) \nonumber\\
          &=g^T(\sum_{i}\frac{1}{\sigma_i(\boldsymbol{H})}u_iu_i^T)(\theta_{k+1}-\theta_{k})\nonumber\\
          &=\sum_{i}\frac{1}{\sigma_i(\boldsymbol{H})}g^T(\theta_{k+1}-\theta_{k}).\nonumber
      \end{aligned}

   If we want to have

   .. math:: g^T(\theta_{k+1}-\theta_{k})\leq g^T\boldsymbol{H}^{-1}(\theta_{k+1}-\theta_{k})\leq -\frac{1}{\eta}||\theta_{k+1}-\theta_{k}||^2_2,

   then every singular value :math:`\sigma_i(\boldsymbol{H})` of
   :math:`\boldsymbol{H}` needs to be smaller than :math:`1`, and hence
   :math:`\sigma_\mathrm{max}(\boldsymbol{H})\leq1,` which justifies the
   assumption we use to prove the bound. ◻

To make the objective value for PCPO with the KL divergence projection
improves, the right hand side of Eq. (`[eq:bound1] <#eq:bound1>`__)
needs to be negative. Hence we have
:math:`\frac{L\eta}{2}\boldsymbol{I}\prec\boldsymbol{H},` implying that
:math:`\sigma_\mathrm{min}(\boldsymbol{H})>\frac{L\eta}{2}.` And to make
the objective value for PCPO with the L2 norm projection improves, the
right hand side of Eq. (`[eq:bound2] <#eq:bound2>`__) needs to be
negative. Hence we have :math:`\eta<\frac{2}{L},` implying that

.. math::

   \begin{aligned}
       &\eta = \sqrt{\frac{2\delta}{g^T\boldsymbol{H}^{-1}g}}<\frac{2}{L}\nonumber\\
        \Rightarrow& \frac{2\delta}{g^T\boldsymbol{H}^{-1}g} < \frac{4}{L^2} \nonumber\\
        \Rightarrow& \frac{g^{T}\boldsymbol{H}^{-1}g}{2\delta}>\frac{L^2}{4}\nonumber\\
        \Rightarrow& \frac{L^2\delta}{2}<g^T\boldsymbol{H}^{-1}g\nonumber\\
        &\leq||g||_2||\boldsymbol{H}^{-1}g||_2\nonumber\\
        &\leq||g||_2||\boldsymbol{H}^{-1}||_2||g||_2\nonumber\\
        &=\sigma_\mathrm{max}(\boldsymbol{H}^{-1})||g||^2_2\nonumber\\
        &=\sigma_\mathrm{min}(\boldsymbol{H})||g||^2_2\nonumber\\
        \Rightarrow&\sigma_\mathrm{min}(\boldsymbol{H})>\frac{L^2\delta}{2||g||^2_2}.
        \label{eq:bound_3}
   \end{aligned}

By the definition of the condition number and Eq.
(`[eq:bound_3] <#eq:bound_3>`__), we have

.. math::

   \begin{aligned}
       \frac{1}{\sigma_\mathrm{min}(\boldsymbol{H})}&<\frac{2||g||_2^2}{L^2\delta}\nonumber\\
   \Rightarrow\frac{\sigma_\mathrm{max}(\boldsymbol{H})}{\sigma_\mathrm{min}(\boldsymbol{H})}&<\frac{2||g||^2_2\sigma_\mathrm{max}(\boldsymbol{H})}{L^2\delta}\nonumber\\
   &\leq \frac{2||g||_2^2}{L^2\delta},\nonumber
   \end{aligned}

which justifies what we discuss.